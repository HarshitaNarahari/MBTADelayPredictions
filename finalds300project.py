# -*- coding: utf-8 -*-
"""FinalDS300Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ltkVDCyBhJ3Z-OF5kR1StOJ6pIlO9NVT

# **Data and Analysis Plan: MBTA Made Easy**

- Nitya Gopalakrishnan
- Elaina Kreher
- Harshita Narahari

## Project Goal:

The goal of our project is to explore the connection between wait times and how delayed a train is in the MBTA. To achieve this goal, we have chosen to focus on the travel times, dwell or wait times, and past alerts. By finding informtion about travel times and dwell times we will be able analyze whether there is a pattern between popular train routes. Hopefully, this will allow us to answer the following questions:
- Can we determine certain features that cause a train route to be more popular than others?
- Are there trends or patterns in the trains that are constantly delayed?

To find our data we will be using "https://www.mbta.com/developers/v3-api" which is the MBTA V3 API and a csv file from the MBTA Open Data Portal "https://mbta-massdot.opendata.arcgis.com/datasets/MassDOT::rapid-transit-and-bus-prediction-accuracy-data/explore"

## Pipeline Overview

For the MBTA V3 API:
- get_routes()
 - requests the MBTA data for a given route (Red, Green, Orange, Blue, Purple lines)
- get_MBTA_data()
 - using get_routes() function, accesses the features of the data that will help determine the popularity of a MBTA train route
"""

# you might use the below modules on this lab
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)

import requests
import json
import pandas as pd

def get_routes(route):
    """ gets the MBTA route API information for a given subway line

    Args:
        route (str): the given MBTA subway line (red, orange, green, blue, purple)

    Returns:
        route_dict (dictionary): a dictionary that contains the API information in JSON form, as well as the arguments
    """
    # url from the mbta api
    url = f"https://api-v3.mbta.com/vehicles?include=route&fields[route]={route}"
    response = requests.get(url)
    route_dict = json.loads(response.text)
    return route_dict

get_routes("Red Line")

def get_mbta_data(route_dict):
    """ returns a dataframe of the dictionary that has API route information for a given MBTA subway line

    Args:
        route_dict (str): a dictionary that contains the API information in JSON form, as well as the arguments

    Returns:
        route_df (dictionary): a dataframe that contains the API information for a given MBTA subway line
    """

    # final dictionary that will be returned
    data_dict = {
        'label': [],
        'current_status': [],
        'occupancy_status': [],
        'lat_long': [],
        'direction_id': [],
        'related_route': [],
        'stop_sequence': [],
    }

    # creating a dataframe of the given dict by looping through the jsons
    for item in route_dict['data']:
         attributes = item['attributes']
         data_dict['label'].append(attributes['label'])
         data_dict['current_status'].append(attributes['current_status'])
         data_dict['occupancy_status'].append(attributes['occupancy_status'])
         data_dict['lat_long'].append((attributes['latitude'], attributes['longitude'])),
         data_dict['direction_id'].append(attributes['direction_id'])
         data_dict['related_route'].append(item['relationships']['route']['data']['id'])
         data_dict['stop_sequence'].append(attributes['current_stop_sequence'])

    route_df = pd.DataFrame(data_dict)

    return route_df

get_mbta_data(get_routes("Blue Line")).head(10)

"""## Supplemental Data Overview

In addition to the MBTA API data, we chose to also use a CSV file containing information about how long a train route was on average and whether the MBTA's prediction of that route was accurate. We felt this was important data to utilize in our project because one of the biggest problems with the MBTA is that their schedules are always either running too early or too late. By using this dataset, we will have access to the average route duration and the accuracy score of what the MBTA predicted the average duration would be.
To use this csv file:
- We will convert the csv file into a dataframe
- get_clean_data()
  - Remove missing values, keep only features that are relevant to our project, add useful columns
"""

predictions = pd.read_csv('mbtapredictions.csv')
predictions.head(10)

"""## Data Cleaning"""

def get_clean_data(df):
    """ returns a dataframe of the cleaned data that comes from the mbtapredictions csv file

    Args:
        df (dataframe): a dataframe of the mbtapredictions csv file

    Returns:
        clean_df (dictionary): a dataframe that contains the mbtapredictions information but with no missing data,
                               only for subways, and an added column for the accuracy score
    """
    # for our project we are only focusing on mbta subway data
    clean_df = df[(df['mode'] == 'subway')]
    # we do not need this id because it is just the count
    clean_df = clean_df.drop('ObjectId', axis = 1)
    # add a column calculating the accuracy score of the predictions
    clean_df['accuracy_score'] = df['num_accurate_predictions']/df['num_predictions']
    return clean_df

get_clean_data(predictions).head(10)

"""## Data Visualizations"""

import plotly
import plotly.express as px
import seaborn as sns
import matplotlib.pyplot as plt

# data for the red, green, and orange lines
red_line = get_mbta_data(get_routes("Red Line"))
green_line = get_mbta_data(get_routes("Green Line"))
orange_line = get_mbta_data(get_routes("Orange Line"))

# pair plot of the stop sequence, direction id, and current status
sns.pairplot(data=red_line, hue='current_status')
plt.show()

"""This visualization plots the Stop Sequence and Direction ID of the train in terms of current status for the Red Line."""

# pair plot of the current status, occupancy status, and stop sequence
sns.pairplot(data=red_line, hue='stop_sequence')
plt.show()

"""This visualization plots the Current Status of the train and Occupancy Status of the train for the Orange Line."""

# aggregates all the subway lines into one dataframe
all_lines_data = pd.concat([red_line, green_line, orange_line], ignore_index=True)
delayed_data = all_lines_data[all_lines_data['current_status'] != 'STOPPED_AT']
average_delays = delayed_data.groupby('stop_sequence')['label'].count().reset_index()

# plots a graph of the average delay times by stop sequence
plt.figure(figsize=(10, 6))
sns.lineplot(x='stop_sequence', y='label', data=average_delays, color='blue')
plt.title('Average Delay by Stop Sequence (Red, Green, Orange Lines)')
plt.xlabel('Stop Sequence')
plt.ylabel('Average Delay Count')
plt.grid(True)
plt.show()

"""## Analysis/ML Plan

In the context of analyzing MBTA data to explore connections between wait times and train delays, several machine learning tools and techniques can be considered. One of the fundamental tools might involve regression analysis, particularly linear regression or its variations. Linear regression assumes a linear relationship between variables, which in this case could help analyze how wait times relate to train delays. However, it assumes that the relationship between the variables is linear, there is minimal multicollinearity (i.e., the features are not highly correlated), and the data is homoscedastic (constant variance).

Another tool that could be employed is clustering algorithms like K-means clustering. Clustering helps identify inherent patterns within the data, such as categorizing various routes based on delay patterns or identifying specific groups of trains with similar characteristics. Clustering methods generally assume that the clusters are spherical, have similar density, and clusters are separable. It's important to preprocess data properly, selecting the right features, and normalizing if required to obtain better clustering results.

Additionally, classification methods like decision trees, random forests, or neural networks might be useful to predict delays or categorize factors influencing popular train routes. These methods make assumptions based on the nature of the data and the problem being solved. Decision trees assume hierarchical relationships in the data and create splits based on feature importance. Random forests further enhance decision trees by aggregating multiple trees, while neural networks handle complex non-linear relationships by processing multiple hidden layers. These techniques often assume the availability of significant amounts of training data and require careful hyperparameter tuning to avoid overfitting or underfitting.

# Random Forest Model

In our analysis, we encountered a challenge when attempting to create a linear regression model to predict delays. The dataset did not include specific information about delays, making it difficult to formulate a regression model. Recognizing the absence of this crucial information, we decided to shift our focus to analyzing the occupancy status of trains. The available data allowed us to explore and predict the occupancy status of trains using a Random Forest model. This approach enables us to understand and predict factors related to the availability of seats on trains, which is valuable information for both commuters and transit operators. While the initial aim was to address delays, adapting our approach to analyze occupancy provides valuable insights into the operational aspects of the transit system.
"""

df = get_mbta_data(get_routes("Blue Line"))

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import LabelEncoder

# Clean data and change to all numerical
df = df.dropna(subset=['occupancy_status'])

label_encoder = LabelEncoder()
df['current_status'] = label_encoder.fit_transform(df['current_status'])

label_encoder = LabelEncoder()
df['occupancy_status'] = label_encoder.fit_transform(df['occupancy_status'])

df

"""0 = FEW_SEATS_AVAILABLE

1 = FULL

2 = MANY_SEATS_AVAILABLE
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer

X = df[['stop_sequence', 'related_route', 'current_status']]  # Features
y = df['occupancy_status']  # Target variable

# Check class distribution
print(df['occupancy_status'].value_counts())

# One-hot encode categorical columns
encoder = OneHotEncoder(drop='first', sparse=False)
X_encoded = pd.DataFrame(encoder.fit_transform(X[['current_status']]), columns=encoder.get_feature_names(['current_status']))
X = pd.concat([X, X_encoded], axis=1).drop(['current_status'], axis=1)

# Drop rows with missing values in 'stop_sequence' and 'related_route'
X = X.dropna(subset=['stop_sequence', 'related_route'])
y = y[X.index]  # Align y with the selected rows in X

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
predictions = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

print(f'Accuracy: {accuracy}')
print('Classification Report:')
print(report)

"""The classification results indicate an overall accuracy of approximately 89.9%, suggesting that the model performs well in predicting the train occupancy status. However, the precision, recall, and F1-score values for the "Few Seats" (0) class are notably lower, with a precision of 1.00, but a recall of only 0.30 and an F1-score of 0.46. This suggests that while the model is highly accurate in identifying situations with many available seats ("Many Seats" class), its performance is suboptimal for scenarios with few available seats. On the other hand, the model exhibits excellent precision, recall, and F1-score for the "Many Seats" (2) class, indicating robust predictive capability in identifying well-occupied trains. The macro and weighted averages reflect a slight imbalance in the model's performance, emphasizing the importance of further refinement, particularly in enhancing the prediction of lower-occupancy scenarios."""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
true_occupancy = y_test
predicted_occupancy = predictions

# Confusion matrix
conf_matrix = confusion_matrix(true_occupancy, predicted_occupancy)

# Plot
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Few Seats', 'Many Seats', 'Full'], yticklabels=['Few Seats', 'Many Seats', 'Full'])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix for Occupancy Prediction')
plt.show()

"""The confusion matrix heatmap provides a comprehensive overview of the model's performance in predicting occupancy status. The diagonal elements represent correct predictions, while off-diagonal elements indicate misclassifications. For the "Few Seats" and "Many Seats" classes, the model struggles, showing zeros for precision, recall, and F1-score. This suggests that the model has difficulty accurately identifying instances with few or many available seats. However, for the "Full" class, the model demonstrates strong performance with a precision of 0.9, recall of 0.92, and an F1-score of 0.91. Despite an overall high accuracy, the model's effectiveness is imbalanced, emphasizing the need for improvements in predicting lower-occupancy scenarios, particularly for "Few Seats" and "Many Seats"."""

